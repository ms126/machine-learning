{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c240f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d7297",
   "metadata": {},
   "source": [
    "Misha Suresh\n",
    "\n",
    "## Q1 Least Square\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3c062941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "  Average Error: 8.549999999999999\n",
      "  Updated Weights: (w0=-0.19, w1=-0.36333333333333334, w2=0.13333333333333333)\n",
      "\n",
      "Iteration 2:\n",
      "  Average Error: 5.261425925925926\n",
      "  Updated Weights: (w0=-0.3278888888888889, w1=-0.6457777777777778, w2=0.23944444444444446)\n",
      "\n",
      "Iteration 3:\n",
      "  Average Error: 3.31300890946502\n",
      "  Updated Weights: (w0=-0.4260851851851852, w1=-0.8660777777777777, w2=0.3245555555555556)\n",
      "\n",
      "Iteration 4:\n",
      "  Average Error: 2.1533203291495204\n",
      "  Updated Weights: (w0=-0.4941011111111111, w1=-1.0386083950617284, w2=0.3934413580246914)\n",
      "\n",
      "Average Weights:\n",
      "  w0: -0.4941011111111111\n",
      "  w1: -1.0386083950617284\n",
      "  w2: 0.3934413580246914\n",
      "Final Average Error: 2.1533203291495204\n"
     ]
    }
   ],
   "source": [
    "data_points = [(1, 1, -0.8), (-1, -2, 0.1), (2, -1, -5.0)]\n",
    "\n",
    "w0 = 0\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "alpha = 0.05\n",
    "T = 4\n",
    "N = len(data_points)\n",
    "\n",
    "def predict(w0, w1, w2, x1, x2):\n",
    "    return w0 + w1 * x1 + w2 * x2\n",
    "\n",
    "# Gradient descent iterations\n",
    "for iteration in range(T):\n",
    "    avg_error = 0\n",
    "    partial_w0 = 0\n",
    "    partial_w1 = 0\n",
    "    partial_w2 = 0\n",
    "\n",
    "    # Calculate avg error and partial derivatives\n",
    "    for x1, x2, y in data_points:\n",
    "        y_hat = predict(w0, w1, w2, x1, x2)\n",
    "        avg_error += (y_hat - y)**2\n",
    "        partial_w0 += 2 * (y_hat - y)\n",
    "        partial_w1 += 2 * (y_hat - y) * x1\n",
    "        partial_w2 += 2 * (y_hat - y) * x2\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w0 -= alpha * (1/N) * partial_w0\n",
    "    w1 -= alpha * (1/N) * partial_w1\n",
    "    w2 -= alpha * (1/N) * partial_w2\n",
    "\n",
    "    print(f\"Iteration {iteration + 1}:\")\n",
    "    print(f\"  Average Error: {avg_error / N}\")\n",
    "    print(f\"  Updated Weights: (w0={w0}, w1={w1}, w2={w2})\")\n",
    "    print()\n",
    "\n",
    "print(\"Average Weights:\")\n",
    "print(f\"  w0: {w0}\")\n",
    "print(f\"  w1: {w1}\")\n",
    "print(f\"  w2: {w2}\")\n",
    "print(f\"Final Average Error: {total_error / N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea59fcb",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "$X^T(y-Xw) = 0 = X^Ty - X^TXw = 0 = X^Ty = X^TXw =$\n",
    "\n",
    "**Exact optimal width:** $w = (X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a2122198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-form solution:\n",
      "  w0: 0.18571428571428494\n",
      "  w1: -2.0571428571428565\n",
      "  w2: 1.0714285714285712\n",
      "Avg Closed Form Error: 4.953491816963971e-31\n"
     ]
    }
   ],
   "source": [
    "data_points = np.array([(1, 1, -0.8), (-1, -2, 0.1), (2, -1, -5.0)])\n",
    "\n",
    "X = data_points[:, :2]\n",
    "y = data_points[:, 2]\n",
    "\n",
    "\n",
    "X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "# Closed-form\n",
    "w = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y\n",
    "\n",
    "w0, w1, w2 = w\n",
    "\n",
    "# Calculate the total error\n",
    "avg_error_closed_form = np.mean((X_bias @ w - y)**2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Closed-form solution:\")\n",
    "print(f\"  w0: {w0}\")\n",
    "print(f\"  w1: {w1}\")\n",
    "print(f\"  w2: {w2}\")\n",
    "print(f\"Avg Closed Form Error: {avg_error_closed_form}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f711e0",
   "metadata": {},
   "source": [
    "## Q2 Perceptrons\n",
    "### a)\n",
    "\n",
    "initial w = $[0, 0]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "666260da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron made a mistake at iteration 1. Weights before update: w = [0. 0.]\n",
      "Iteration 1: w = [1. 1.]\n",
      "Perceptron made a mistake at iteration 2. Weights before update: w = [1. 1.]\n",
      "Iteration 2: w = [-1.  2.]\n",
      "Iteration 3: w = [-1.  2.]\n",
      "Iteration 4: w = [-1.  2.]\n",
      "The binary perceptron correctly classifies all data points with the final values of the weights: False\n"
     ]
    }
   ],
   "source": [
    "data_points = np.array([[1, 1, 1],\n",
    "                        [2, -1, -1],\n",
    "                        [-3, -1, 1],\n",
    "                        [-3, 1, 1]])\n",
    "\n",
    "#Initialized weights\n",
    "w = np.zeros(2)\n",
    "\n",
    "#Perceptron algorithm: \n",
    "for i in range(len(data_points)):\n",
    "    x = data_points[i, :2]\n",
    "    y = data_points[i, 2]\n",
    "    #print(x, y)\n",
    "    y_pred = np.sign(w @ x)\n",
    "    \n",
    "    #if perceptron makes mistake:\n",
    "    if y_pred != y:\n",
    "        print(f\"Perceptron made a mistake at iteration {i+1}. Weights before update: w = {w}\")\n",
    "        w = w + y * x\n",
    "    \n",
    "    #print each iteration\n",
    "    print(f\"Iteration {i+1}: w = {w}\")\n",
    "    \n",
    "# Check - does binary perceptron correctly classify all data points with the final values of the weights?\n",
    "correct = all(\n",
    "    np.all(np.sign(np.dot(w, x)) == y_true)\n",
    "    for x, y_true in data_points[:, :2])\n",
    "\n",
    "print(f\"The binary perceptron correctly classifies all data points with the final values of the weights: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211e5d2",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e905c38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron made a mistake at iteration 2. Weights before update: w = [0. 0.]\n",
      "Iteration 2: w = [-2.  1.]\n",
      "Perceptron made a mistake at iteration 1. Weights before update: w = [-2.  1.]\n",
      "Iteration 1: w = [-1.  2.]\n",
      "Iteration 3: w = [-1.  2.]\n",
      "Iteration 4: w = [-1.  2.]\n",
      "The binary perceptron correctly classifies all data points with the final values of the weights: False\n"
     ]
    }
   ],
   "source": [
    "data_points = np.array([[1, 1, 1],\n",
    "                        [2, -1, -1],\n",
    "                        [-3, -1, 1],\n",
    "                        [-3, 1, 1]])\n",
    "\n",
    "#Initialized weights\n",
    "w = np.zeros(2)\n",
    "\n",
    "#Perceptron algorithm: \n",
    "for i in [2, 1, 3, 4]:\n",
    "    x = data_points[i - 1, :2]\n",
    "    y = data_points[i - 1, 2]\n",
    "    #print(x, y)\n",
    "    y_pred = np.sign(w @ x)\n",
    "    \n",
    "    #if perceptron makes mistake:\n",
    "    if y_pred != y:\n",
    "        print(f\"Perceptron made a mistake at iteration {i}. Weights before update: w = {w}\")\n",
    "        w = w + y * x\n",
    "    \n",
    "    #print each iteration\n",
    "    print(f\"Iteration {i}: w = {w}\")\n",
    "    \n",
    "# Check - does binary perceptron correctly classify all data points with the final values of the weights?\n",
    "correct = all(\n",
    "    np.all(np.sign(np.dot(w, x)) == y_true)\n",
    "    for x, y_true in data_points[:, :2])\n",
    "\n",
    "print(f\"The binary perceptron correctly classifies all data points with the final values of the weights: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a6714",
   "metadata": {},
   "source": [
    "## Q3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4f785",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Log-likelihood function = $maxll(w) = max \\frac{1}{n}\\sum_{i=1}^{n}logP(y^i | x^i;w)$ where,\n",
    "\n",
    "Positive = $P(y^i = +1 | x^i;w) = \\frac{1}{1+e^{-w*x^i}}$\n",
    "\n",
    "Negative = $P(y^i = -1 | x^i;w) = 1 - \\frac{1}{1+e^{-w*x^i}}$\n",
    "\n",
    "start with $w[0,0]^T$ and, \n",
    "\n",
    "$w_1 = w_1 + \\alpha \\frac{\\partial ll(w)}{\\partial w_1}$ \n",
    "\n",
    "$w_2 = w_2 + \\alpha \\frac{\\partial ll(w)}{\\partial w_2}$\n",
    "\n",
    "\n",
    "**The gradient formulation for $[\\frac{\\partial ll(w)}{\\partial w_1} \\frac{\\partial ll(w)}{\\partial w_2} ]$ with respect to the training set is:**\n",
    "\n",
    "**for $w_1:$** $\\frac{\\partial ll(w)}{\\partial w_1} = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ -\\frac{1}{1 + e^{-w \\cdot x}} \\cdot e^{-w \\cdot x} \\right]$\n",
    "\n",
    "\n",
    "**for $w_2:$** $\\frac{\\partial ll(w)}{\\partial w_2} = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ -x_2 \\cdot \\frac{1}{1 + e^{-w \\cdot x}} \\cdot e^{-w \\cdot x} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f564b5c",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "4 iterations of gradient ascent with $\\alpha = 0.01$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "256facab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: w = [-0.00749987  0.00499983], Avg. Log-Likelihood = -0.6550818404328415\n",
      "Iteration 2: w = [-0.01499919  0.00999914], Avg. Log-Likelihood = -0.6170538987336612\n",
      "Iteration 3: w = [-0.02249745  0.01499743], Avg. Log-Likelihood = -0.5790662168753029\n",
      "Iteration 4: w = [-0.02999413  0.01999418], Avg. Log-Likelihood = -0.5411216327028452\n"
     ]
    }
   ],
   "source": [
    "data_points = np.array([[1, 1, -0.8], [-1, -2, 0.1], [2, -1, -5.0]])\n",
    "w = np.zeros(2)\n",
    "n = len(data_points)\n",
    "alpha = 0.01\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def log_likelihood(w, data):\n",
    "    ll = 0\n",
    "    for i in range(len(data)):\n",
    "        x = data[i, :2]\n",
    "        y = data[i, 2]\n",
    "        p_pos = sigmoid(np.dot(w, x))\n",
    "        p_neg = 1 - p_pos\n",
    "        ll += y * np.log(p_pos) + (1 - y) * np.log(p_neg)\n",
    "    return ll / n\n",
    "\n",
    "for iteration in range(4):\n",
    "    for i in range(len(data_points)):\n",
    "        x = data_points[i, :2]\n",
    "        y = data_points[i, 2]\n",
    "        p_pos = sigmoid(np.dot(w, x))\n",
    "        gradient_w1 = -p_pos * np.exp(-np.dot(w, x)) / (1 + np.exp(-np.dot(w, x)))\n",
    "        gradient_w2 = -x[1] * p_pos * np.exp(-np.dot(w, x)) / (1 + np.exp(-np.dot(w, x)))\n",
    "        w[0] += alpha * gradient_w1\n",
    "        w[1] += alpha * gradient_w2\n",
    "    avg_ll = log_likelihood(w, data_points)\n",
    "    print(f\"Iteration {iteration + 1}: w = {w}, Avg. Log-Likelihood = {avg_ll}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa81407",
   "metadata": {},
   "source": [
    "4 iterations of gradient ascent with $\\alpha = 0.2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6aa51cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: w = [-0.05955009  0.09944214], Avg. Log-Likelihood = -0.2940555583534838\n",
      "Iteration 2: w = [-0.12224628  0.19797277], Avg. Log-Likelihood = 0.11027554544309821\n",
      "Iteration 3: w = [-0.18803904  0.29356081], Avg. Log-Likelihood = 0.5166077282085889\n",
      "Iteration 4: w = [-0.25677929  0.38452964], Avg. Log-Likelihood = 0.9222621760179351\n"
     ]
    }
   ],
   "source": [
    "data_points = np.array([[1, 1, -0.8], [-1, -2, 0.1], [2, -1, -5.0]])\n",
    "w = np.zeros(2)\n",
    "n = len(data_points)\n",
    "alpha = 0.2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def log_likelihood(w, data):\n",
    "    ll = 0\n",
    "    for i in range(len(data)):\n",
    "        x = data[i, :2]\n",
    "        y = data[i, 2]\n",
    "        p_pos = sigmoid(np.dot(w, x))\n",
    "        p_neg = 1 - p_pos\n",
    "        ll += y * np.log(p_pos) + (1 - y) * np.log(p_neg)\n",
    "    return ll / n\n",
    "\n",
    "for iteration in range(4):\n",
    "    for i in range(len(data_points)):\n",
    "        x = data_points[i, :2]\n",
    "        y = data_points[i, 2]\n",
    "        p_pos = sigmoid(np.dot(w, x))\n",
    "        gradient_w1 = -p_positive * np.exp(-np.dot(w, x)) / (1 + np.exp(-np.dot(w, x)))\n",
    "        gradient_w2 = -x[1] * p_pos * np.exp(-np.dot(w, x)) / (1 + np.exp(-np.dot(w, x)))\n",
    "        w[0] += alpha * gradient_w1\n",
    "        w[1] += alpha * gradient_w2\n",
    "    avg_ll = log_likelihood(w, data_points)\n",
    "    print(f\"Iteration {iteration + 1}: w = {w}, Avg. Log-Likelihood = {avg_ll}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9083c5",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "As we can see from the results above, the value of alpha severly affects the weights and the average log-likelihood. When $\\alpha = 0.01$ we can see that it will take more iterations to reach the optimal solution, hence a slower convergence. Additionally, when $\\alpha = 0.01$, we can see that the average log-likelihood values decrease with each iteration. On the other hand, when $\\alpha = 0.2$, we can see that it results in a faster and more efficient convergence. We can also see that the average log-likelihood values increase with each iteration, which represents a stronger $\\alpha$ value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400ded4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
